https://ai.facebook.com/blog/wav2vec-20-learning-the-structure-of-speech-from-raw-audio/

# Wav2vec 2.0: Learning the structure of speech from raw audio (Article) #

There are thousands of languages spoken around the world, many with several different dialects, which presents a huge challenge for building high-quality speech recognition technology. It's simply not feasible to obtain many possible domains (read speech, telephone speech, etc.).

Wav2vec 2.0 uses self-supervision to push the boundaries by learning from unlabeled training data to enable speech recognition systems for many more languages, dialects, and domains.

Speech audio is a continuous signal that captures many aspects of the recording with no clear segmentation into words or other units.

## Learning discrete latent speech units ##

The model first processes the raw waveform of the speech audio with a multilayer convolutional neural network to get latent audio representations of 25ms each. These representations are then fed into a quantizer as well as a transformer.

- The quantizer chooses a speech unit for the latent audio representation from an inventory of learned units. About half the audio representations are masked before being fed into the transformer.

- The transformer adds information from the entire audio sequence.

Finally the output of the transformer is used to solve a contrastive task. This task requires the model to identify the correct quantized speech units for the masked positions.

## Cross-lingual training ##

The idea is to pretrain a single model on multiple languages at the same time, which results in representations that are better than training on a single language. 