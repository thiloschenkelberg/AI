https://www.ibm.com/cloud/learn/convolutional-neural-networks

## What are convolutional neural networks ##

Neural networks are a subset of machine learning, and they are at the heart of deep learning algorithms. They are comprised of node layers, containing an input layer, one or more hidden layers, and an output layer. Each node connects to each other and has an associated weight and threshold. If the output of any individual node is above the specified threshold value, that node is activated, sending data to the next layer of the network. Otherwise no data is passed along to the next layer of the network.

Convolutional neural networks (ConvNets or CNNs) are more often utilized for classification and computer vision tasks. Convolutional neural networks now provide a more scalable approach to image classification and object recognition tasks, leveraging principles from linear algebra, specifically matrix multiplication, to identify patterns within an image. They can be computationally demanding, requiring graphical processing units (GPUs) to train models.

## How do convolutional neural networks work? ##

They have three main types of layers:

- Convolutional layer

- Pooling layer

- Fully-connected (FC) layer

With each layer, the CNN increases in its complexity, identifying greater portions of the image. Earlier layers focus on simple features, such as colors and edges. As the image data progresses through the layers of the CNN, it starts to recognize larger elements or shapes of the object until it finally identifies the intended object.

### Convolutional layer ###

The convolutional layer is the core building block of a CNN, and it is where the majority of computation occurs. It requires a few components, which are input data, a filter, and a feature map.

The convolutional layer has a feature detector, also known as a kernel or filter, which will move across the receptive fields of the image, checking if the feature is present. This process is known as a convolution.


The feature detector is a two-dimensional (2D) array of weights, which represents part of the image. The filter is applied to an area of the image, and a dot product is calculated between the input pixels and the filter. This dot product is then fed into an output array. Afterwards, the filter shifts by a stride, repeating the process until the kernel has swept across the entire image. The final output from the series of dot products from the input and the filter is known as a feature map, activation map, or a convolved feature. The output array does not need to map directly to each input value, convolutional (and pooling) layers are commonly referred to as "partially connected" layers. However, this characteristic can also be described as local connectivity.

Weights in the feature detector remain fixed as it moves across the image, which is also known as parameter sharing. Some parameters, like the weight values, adjust during training through the process of backpropagation and gradient descent. There are three hyperparameters which affect the volume size of the output that need to be set before the training of the neural network beings:

1. The number of filters affects the depth of the output.

2. Stride is the distance, or number of pixels, that the kernel moves over the input matrix. While stride values of two or greater is rare, a larger stride yields a smaller output.

3. Zero-padding is usually used when the filters do not fit the input image. This sets all elements that fall outside of the input matrix to zero, producing a larger or equally sized output. There are three types of padding:

    - Valid padding:

    - Same padding:

    - Full padding:



