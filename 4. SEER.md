https://ai.facebook.com/blog/seer-the-start-of-a-more-powerful-flexible-and-accessible-era-for-computer-vision/

Paper: https://arxiv.org/pdf/2103.01988.pdf

# SEER: The start of a more powerful, flexible, and accessible era for computer vision (Article) #

We've developed SEER (SElf-supERvised), a new billion-parameter self-supervised computer vision model that can learn from any random group of images on the internet - without the need for careful curation and labeling that goes into most computer vision training today.

SEER's performance demonstrates that self-supervised learning can excel at computer vision tasks in real-world settings.

## Self-supervised computer vision in the real world ##

Successfully scaling models to work efficiently with complex high-dimensional image data required two key components:

1. An algorithm that could learn from a vast number of random images without any metadata or annotations

2. A convolutional network (ConvNet) large enough to capture and learn every visual concept from this large and complex data.

We took advantage of a new algorithm called SwAV, which developed from a collaboration between FAIR and Inria to research self-supervised learning. SwAV uses online clustering to rapidly group images with similar visual concepts and leverage their similarities.

A recent innovation by FAIR in the realm of architecture design led to a new model family called RegNets that perfectly fit these needs. RegNet models are ConvNets capable of scaling to billions or potentially even trillions of parameters, and can be optimized to fit different runtime and memory limitations.

## VISSL ##

VISSL is a PyTorch-based library that allows for self-supervised training at both small and massive scale with a wide variety of modern methods. VISSL also contains an extensive benchmark suite and a model zoo consisting of more than 60 pretrained models, allowing researchers to compare several modern self-supervised methods.

VISSL facilitates self-supervised learning at scale by integrating several existing algorithms that reduce the per-GPU memory requirement and increase the training speed of any given model. VISSL combines:

- Mixed precision from the NVIDIA Apex library: Reduces memory requirements and speeds up runtime

- Gradient checkpointing from PyTorch: allows the model to be trained on large batch sizes by trading compute for memory

- Sharded optimizer from the FairScale library: Significantly reduces memory usage by sharding model optimizer state and gradients

- Dedicated optimizations for online self-supervised training: for example, a constant learning schedule that does not depend on the total number of training parameter updates

## A self-supervised future ##

Eliminating the need for human annotations and metadata enables the computer vision community to work with larger and more diverse data sets, learn from random public images, and potentially mitigate some of the biases that come into play with data curation. Self-supervised learning can also help specialize models in domains where we have limited images or metadata, like medical imaging.

With no labor required up front for labeling, models can be created and deployed quicker, enabling faster and more accurate responses to rapidly evolving situations.